{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7b9de57",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f9cdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7e2878",
   "metadata": {},
   "source": [
    "# Part A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ca3fe3",
   "metadata": {},
   "source": [
    "## Normalization of data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90b82a0",
   "metadata": {},
   "source": [
    "- We first read both files since the dataset was provided as train and test not as one csv file\n",
    "- We then dropped the header row from both tables and concatenated them together to create the dataset\n",
    "- We converted the data into a numpy array for faster operations\n",
    "- We then seperated the labels into `y` and the features in `X` and set their data types\n",
    "- Feature values were normalized to take value between `0 and 1`\n",
    "- Afterwards we split the data into train, validation, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88239003",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"dataset/mnist_test.csv\")\n",
    "data2=pd.read_csv(\"dataset/mnist_train.csv\")\n",
    "data2=data2.drop(data2.index[0])\n",
    "data=data.drop(data.index[0])\n",
    "data=pd.concat([data,data2])\n",
    "\n",
    "data_np = data.to_numpy()\n",
    "\n",
    "# Separate labels (first column) and features (remaining columns)\n",
    "y = data_np[:, 0].astype(int)\n",
    "X = data_np[:, 1:].astype(float)\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "X = X / 255.0\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.4, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7217b1d",
   "metadata": {},
   "source": [
    "  - `reshape()` is a numpy function that changes the dimension of a numpy array without changing the data\n",
    "  - since the images were in the form of a flattened vector, we can change them back into 28x28 images using the `reshape() `\n",
    "  - -1 -> it tells the function to automatically calculate this dimesion according to the other dimensions we will specifiy later (this one returns the number of images)\n",
    "  - 1 -> number of color channels (only 1 since the images are grayscale)\n",
    "  - 28 -> height \n",
    "  - 28 -> width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab586ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reshape for neural networks\n",
    "X_train_nn = X_train.reshape(-1, 1, 28, 28)\n",
    "X_val_nn = X_val.reshape(-1, 1, 28, 28)\n",
    "X_test_nn = X_test.reshape(-1, 1, 28, 28)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac0dd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# For neural networks (keep image shape)\n",
    "X_train_nn_tensor = torch.tensor(X_train_nn, dtype=torch.float32)\n",
    "X_val_nn_tensor = torch.tensor(X_val_nn, dtype=torch.float32)\n",
    "X_test_nn_tensor = torch.tensor(X_test_nn, dtype=torch.float32)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset_flat = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset_flat = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset_flat = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_dataset_nn = TensorDataset(X_train_nn_tensor, y_train_tensor)\n",
    "val_dataset_nn = TensorDataset(X_val_nn_tensor, y_val_tensor)\n",
    "test_dataset_nn = TensorDataset(X_test_nn_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 64\n",
    "\n",
    "train_loader_flat = DataLoader(train_dataset_flat, batch_size=batch_size, shuffle=True)\n",
    "val_loader_flat = DataLoader(val_dataset_flat, batch_size=batch_size, shuffle=False)\n",
    "test_loader_flat = DataLoader(test_dataset_flat, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_loader_nn = DataLoader(train_dataset_nn, batch_size=batch_size, shuffle=True)\n",
    "val_loader_nn = DataLoader(val_dataset_nn, batch_size=batch_size, shuffle=False)\n",
    "test_loader_nn = DataLoader(test_dataset_nn, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8aec4ab",
   "metadata": {},
   "source": [
    "## Binary logistic regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940f42f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter only 0 and 1 from training set\n",
    "train_mask = (y_train == 0) | (y_train == 1)\n",
    "val_mask = (y_val == 0) | (y_val == 1)\n",
    "test_mask = (y_test == 0) | (y_test == 1)\n",
    "\n",
    "X_train_bin = X_train_tensor[train_mask]\n",
    "y_train_bin = y_train_tensor[train_mask]\n",
    "\n",
    "X_val_bin = X_val_tensor[val_mask]\n",
    "y_val_bin = y_val_tensor[val_mask]\n",
    "\n",
    "X_test_bin = X_test_tensor[test_mask]\n",
    "y_test_bin = y_test_tensor[test_mask]\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 64\n",
    "train_loader_bin = DataLoader(TensorDataset(X_train_bin, y_train_bin), batch_size=batch_size, shuffle=True)\n",
    "val_loader_bin = DataLoader(TensorDataset(X_val_bin, y_val_bin), batch_size=batch_size, shuffle=False)\n",
    "test_loader_bin = DataLoader(TensorDataset(X_test_bin, y_test_bin), batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e0227c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 784  # flattened MNIST size\n",
    "\n",
    "# Weights & bias\n",
    "W = torch.zeros(input_dim, 1, dtype=torch.float32, requires_grad=True)\n",
    "b = torch.zeros(1, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "learning_rate = 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193364e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + torch.exp(-z))\n",
    "\n",
    "def binary_cross_entropy(pred, target):\n",
    "    # Adding small epsilon for numerical stability\n",
    "    eps = 1e-8\n",
    "    return -(target * torch.log(pred + eps) + (1 - target) * torch.log(1 - pred + eps)).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb3db49",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefd2c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_sum = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            y_batch = y_batch.unsqueeze(1).float()\n",
    "            logits = X_batch @ W + b\n",
    "            y_pred = sigmoid(logits)\n",
    "            loss = binary_cross_entropy(y_pred, y_batch)\n",
    "            predicted = (y_pred >= 0.5).int()\n",
    "            correct += (predicted.squeeze() == y_batch.squeeze()).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "            loss_sum += loss.item()\n",
    "    return loss_sum / len(loader), correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf56dde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, val_losses = [], []\n",
    "train_accs, val_accs = [], []\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for X_batch, y_batch in train_loader_bin:\n",
    "        y_batch = y_batch.unsqueeze(1).float()\n",
    "        logits = X_batch @ W + b\n",
    "        y_pred = sigmoid(logits)\n",
    "\n",
    "        loss = binary_cross_entropy(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            W -= learning_rate * W.grad\n",
    "            b -= learning_rate * b.grad\n",
    "\n",
    "        W.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        predicted = (y_pred >= 0.5).int()\n",
    "        correct += (predicted.squeeze() == y_batch.squeeze()).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "    train_loss = total_loss / len(train_loader_bin)\n",
    "    train_acc = correct / total\n",
    "    val_loss, val_acc = evaluate(val_loader_bin)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47550d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "# Loss curves\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy curves\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(train_accs, label='Train Accuracy')\n",
    "plt.plot(val_accs, label='Validation Accuracy')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training & Validation Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724b3735",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader_bin:\n",
    "        logits = X_batch @ W + b\n",
    "        y_pred = sigmoid(logits)\n",
    "        preds = (y_pred >= 0.5).int().squeeze().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(y_batch.numpy())\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=[0,1], yticklabels=[0,1])\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix (Test Set)\")\n",
    "plt.show()\n",
    "\n",
    "test_acc = np.mean(np.array(all_preds) == np.array(all_labels))\n",
    "print(f\"Final Test Accuracy: {test_acc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74bac60",
   "metadata": {},
   "source": [
    "## Softmax Regression Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095ecec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the full dataset for multi-class classification (digits 0â€“9)\n",
    "X_train_mc = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_mc = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_mc = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_mc = torch.tensor(y_val, dtype=torch.long)\n",
    "X_test_mc = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_mc = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "num_features = X_train.shape[1]\n",
    "num_classes = 10\n",
    "\n",
    "W = torch.randn(num_features, num_classes, dtype=torch.float32, requires_grad=True)\n",
    "b = torch.zeros(num_classes, dtype=torch.float32, requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9a6e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    exp_z = torch.exp(z - z.max(dim=1, keepdim=True).values)  # numerical stability\n",
    "    return exp_z / exp_z.sum(dim=1, keepdim=True)\n",
    "\n",
    "def cross_entropy(pred, target):\n",
    "    # target is a vector of class indices\n",
    "    n = target.shape[0]\n",
    "    eps = 1e-8\n",
    "    log_likelihood = -torch.log(pred[range(n), target] + eps)\n",
    "    return log_likelihood.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f30531",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "epochs = 50\n",
    "\n",
    "train_losses_mc, val_losses_mc = [], []\n",
    "train_accs_mc, val_accs_mc = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    logits = X_train_mc @ W + b\n",
    "    probs = softmax(logits)\n",
    "    loss = cross_entropy(probs, y_train_mc)\n",
    "    \n",
    "    # Backprop\n",
    "    loss.backward()\n",
    "    \n",
    "    # Gradient descent\n",
    "    with torch.no_grad():\n",
    "        W -= lr * W.grad\n",
    "        b -= lr * b.grad\n",
    "        W.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "    \n",
    "    # Training accuracy\n",
    "    train_pred = probs.argmax(dim=1)\n",
    "    train_acc = (train_pred == y_train_mc).float().mean().item()\n",
    "    \n",
    "    # Validation\n",
    "    with torch.no_grad():\n",
    "        val_logits = X_val_mc @ W + b\n",
    "        val_probs = softmax(val_logits)\n",
    "        val_loss = cross_entropy(val_probs, y_val_mc)\n",
    "        val_pred = val_probs.argmax(dim=1)\n",
    "        val_acc = (val_pred == y_val_mc).float().mean().item()\n",
    "    \n",
    "    train_losses_mc.append(loss.item())\n",
    "    val_losses_mc.append(val_loss.item())\n",
    "    train_accs_mc.append(train_acc)\n",
    "    val_accs_mc.append(val_acc)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}, Val Acc: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0e83fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(train_losses_mc, label=\"Train Loss\")\n",
    "plt.plot(val_losses_mc, label=\"Val Loss\")\n",
    "plt.legend(); plt.title(\"Softmax Regression Loss\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(train_accs_mc, label=\"Train Accuracy\")\n",
    "plt.plot(val_accs_mc, label=\"Validation Accuracy\")\n",
    "plt.legend(); plt.title(\"Softmax Regression Accuracy\")\n",
    "\n",
    "plt.show()\n",
    "with torch.no_grad():\n",
    "    test_logits = X_test_mc @ W + b\n",
    "    test_probs = softmax(test_logits)\n",
    "    test_pred = test_probs.argmax(dim=1)\n",
    "    test_acc = (test_pred == y_test_mc).float().mean().item()\n",
    "\n",
    "print(f\"Final Test Accuracy: {test_acc*100:.2f}%\")\n",
    "\n",
    "cm = confusion_matrix(y_test_mc, test_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Softmax Regression Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPer-Class Accuracy:\")\n",
    "print(classification_report(y_test_mc, test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a6f74a",
   "metadata": {},
   "source": [
    "# Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92714c4",
   "metadata": {},
   "source": [
    "### Custom Neural Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e617f1f2",
   "metadata": {},
   "source": [
    "The **FullyConnectedNN** class implements the ANN. My implementation allows for different depths of the linear NN layers. Some important functions used:\n",
    "- **nn.Linear**: instantiates a linear transformation layer, that applies the formula `y = xW^T + b`\n",
    "- **nn.init.kaiming_normal_**: initialises the weights for the layers that use **ReLU** activation function and its variants to account for the fact that on average 50% of the weight would be 0, as **`RelU(x) = max(0, x)`**\n",
    "- **nn.init.xavier_normal_**: initialises the weights for the layers that use activation functions that have zero mean and unit variance, such as the sigmoid function that is used in softmax regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe317c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedNN(nn.Module):\n",
    "    def __init__(self, input_size=784, hidden_sizes=[128, 64], num_classes=10):\n",
    "        super(FullyConnectedNN, self).__init__()\n",
    "    \n",
    "        layers = []\n",
    "        in_features = input_size\n",
    "        \n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(in_features, h))\n",
    "            in_features = h\n",
    "        \n",
    "        layers.append(nn.Linear(in_features, num_classes))\n",
    "        \n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "        for layer in self.layers[:-1]:\n",
    "            nn.init.kaiming_normal_(layer.weight)\n",
    "        nn.init.xavier_normal_(self.layers[-1].weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.ndim == 4:\n",
    "            x = x.view(x.size(0), -1)\n",
    "\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = F.relu(layer(x))\n",
    "\n",
    "        return self.layers[-1](x)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e3290e",
   "metadata": {},
   "source": [
    "### Training Infrastructure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd2fc60",
   "metadata": {},
   "source": [
    "Auto-detection of a GPU and using it if exists. Otherwise, fall back onto CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f30a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training on: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48074cec",
   "metadata": {},
   "source": [
    "The **accuracy** function calculates the accuracy per batch by:\n",
    "- Taking the **outputs**, which are the raw logits, and the **labels**\n",
    "- Getting the prediction by getting the maximum value using **argmax**\n",
    "- Calculating the mean of how many predictions actually matched the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49afea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    preds = outputs.argmax(dim=1)\n",
    "    return (preds == labels).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8a34f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=10, lr=0.01):\n",
    "    model = model.to(device) # Move model to device\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss() # combines softmax and negative log likelihood\n",
    "    \n",
    "    # Stochastic Gradient Descent optimizer\n",
    "    # model.parameters() gives all learnable parameters of the model to be optimized\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    \n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train() # set the model to training mode\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        \n",
    "        for X, y in train_loader: # train the model in batches\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            outputs = model(X) # calls the forward function and returns raw logits\n",
    "            loss = criterion(outputs, y) # computes how far the outputs are from the true labels\n",
    "            \n",
    "            optimizer.zero_grad() # reset gradients before backpropagation to prevent unwanted accumulation\n",
    "            loss.backward() # compute gradients via backpropagation\n",
    "            optimizer.step() # update model parameters using w = w - lr * gradient\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            running_acc += accuracy(outputs, y)\n",
    "        \n",
    "        train_losses.append(running_loss / len(train_loader))\n",
    "        train_accs.append(running_acc / len(train_loader))\n",
    "\n",
    "        model.eval() # set the model to validation mode\n",
    "        val_loss, val_acc = 0.0, 0.0\n",
    "\n",
    "        with torch.no_grad(): # disable gradient calculation during validation\n",
    "            for X, y in val_loader:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                \n",
    "                outputs = model(X)\n",
    "                loss = criterion(outputs, y)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_acc += accuracy(outputs, y)\n",
    "        \n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        val_accs.append(val_acc / len(val_loader))\n",
    "        \n",
    "        # Log progress\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] \"\n",
    "            f\"Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_accs[-1]*100:.2f}% \"\n",
    "            f\"| Val Loss: {val_losses[-1]:.4f}, Val Acc: {val_accs[-1]*100:.2f}%\")\n",
    "    \n",
    "    return train_losses, val_losses, train_accs, val_accs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a08ba8",
   "metadata": {},
   "source": [
    "- Run the fully connected NN model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befb9b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_nn_model = FullyConnectedNN()\n",
    "train_losses, val_losses, train_accs, val_accs = train_model(\n",
    "    fc_nn_model,\n",
    "    train_loader_nn,\n",
    "    val_loader_nn,\n",
    "    epochs=10,\n",
    "    lr=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda5801c",
   "metadata": {},
   "source": [
    "# Part C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c64a7ca",
   "metadata": {},
   "source": [
    "- Performance Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcbdef3",
   "metadata": {},
   "source": [
    "- Hyperparameter Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1993b74",
   "metadata": {},
   "source": [
    "- Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2297f849",
   "metadata": {},
   "source": [
    "# Part D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c12762",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd9bb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicCNN(nn.Module):\n",
    "    def __init__(self, conv_channels=[32, 64], fc_sizes=[128], num_classes=10):\n",
    "        super(DynamicCNN, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        in_channels = 1\n",
    "        \n",
    "        for out_channels in conv_channels:\n",
    "            layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.MaxPool2d(kernel_size=2))\n",
    "            in_channels = out_channels\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(*layers)\n",
    "\n",
    "        pool_count = conv_channels.__len__()\n",
    "        spatial_size = 28 // (2 ** pool_count)\n",
    "\n",
    "        fc_layers = []\n",
    "        in_features = conv_channels[-1] * spatial_size * spatial_size\n",
    "\n",
    "        for h in fc_sizes:\n",
    "            fc_layers.append(nn.Linear(in_features, h))\n",
    "            fc_layers.append(nn.ReLU())\n",
    "            in_features = h\n",
    "\n",
    "        fc_layers.append(nn.Linear(in_features, num_classes))\n",
    "        self.fc_layers = nn.Sequential(*fc_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88798250",
   "metadata": {},
   "source": [
    "- Run the convolutional NN model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7151234",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = DynamicCNN()\n",
    "train_losses, val_losses, train_accs, val_accs = train_model(\n",
    "    cnn_model,\n",
    "    train_loader_nn,\n",
    "    val_loader_nn,\n",
    "    epochs=10,\n",
    "    lr=0.01\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
